# LLM Finetuning and RAG Creation

## Getting Started

To get started with this project, you will need to clone the repository and install the required dependencies. Detailed instructions are provided below.

## Instructions

### 1. Text Extraction

Extract text of interest from children's academic books. This text will be used as the basis for generating prompts and responses in the subsequent steps.

### 2. Prompt Generation

Run the `scientific_facts_prompts_generation` script to generate prompts in the desired format for different LLMs. These prompts are based on the extracted text and are designed to elicit specific types of responses from the models.

### 3. Response Generation

Use the `llm_response_generation` script to generate responses from different LLMs. The responses are stored in response files for further analysis and comparison.

### 4. LLM Finetuning

The `llm_finetuning` script is used to fine-tune the **LLaMA 3.1 8B Instruct** model. This step involves training the model on the generated prompts and responses to improve its performance.

### 5. RAG Creation

Create a RAG for the fine-tuned **LLaMA 3.1 8B Instruct** model using the `llama3_rag` script. The RAG system will combine the model's capabilities with a retrieval mechanism to enhance response accuracy and relevance.

### 6. Response Comparison

Finally, use the `llm_response_comparison` script to compare the responses from the LLaMA RAG output with other models, such as **GPT-3.5**. This comparison will help evaluate the performance improvements achieved through fine-tuning and RAG creation.

## Dependencies

To install the necessary dependencies, run:

```bash
pip install -r requirements.txt
